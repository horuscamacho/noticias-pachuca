# Plan de ImplementaciÃ³n Detallado - Sistema Anti-DetecciÃ³n
## Scraping con RotaciÃ³n de Proxies para NestJS Backend

**Proyecto:** Noticias Pachuca - Backend API
**Fecha:** 21 de Octubre de 2025
**Elaborado por:** Jarvis (Backend Architect + Technical Researcher)

---

## Ãndice

1. [Preguntas Clave Antes de Arrancar](#preguntas-clave-antes-de-arrancar)
2. [VisiÃ³n General](#visiÃ³n-general)
3. [AnÃ¡lisis del Sistema Actual](#anÃ¡lisis-del-sistema-actual)
4. [Arquitectura Propuesta](#arquitectura-propuesta)
5. [Plan de ImplementaciÃ³n por Fases](#plan-de-implementaciÃ³n-por-fases)
6. [Checklist Detallado](#checklist-detallado)
7. [Resumen Ejecutivo por Fase](#resumen-ejecutivo-por-fase)
8. [MÃ©tricas de Ã‰xito](#mÃ©tricas-de-Ã©xito)

---

## Preguntas Clave Antes de Arrancar

### ğŸš¦ IMPORTANTE: Responder ANTES de Iniciar la ImplementaciÃ³n

Estas preguntas definen el alcance, presupuesto y estrategia del proyecto. Es fundamental responderlas antes de empezar cualquier fase de desarrollo.

---

#### 1. Â¿QuÃ© estrategia de implementaciÃ³n prefieres?

**OpciÃ³n A: RÃPIDA (Empezar con lo bÃ¡sico)**
- âœ… **Alcance:** Solo Fase 0 + Fase 1 (Quick Wins)
- â±ï¸ **DuraciÃ³n:** 9-12 horas total
- ğŸ’° **Costo:** $0
- ğŸ“ˆ **Mejora esperada:** +10-15% en tasa de Ã©xito inmediata
- ğŸ¯ **Ideal para:** Validar el concepto antes de invertir mÃ¡s tiempo
- **DespuÃ©s:** Evaluar resultados y decidir si continuar con mÃ¡s fases

**OpciÃ³n B: GRADUAL (Recomendada)**
- âœ… **Alcance:** ImplementaciÃ³n por etapas con validaciÃ³n
  - Etapa 1: Fase 0+1 â†’ Validar â†’ Aprobar siguiente etapa
  - Etapa 2: Fase 2+3 â†’ Validar â†’ Aprobar siguiente etapa
  - Etapa 3: Fase 4+5+6+7 â†’ Deploy a producciÃ³n
- â±ï¸ **DuraciÃ³n:** 8-12 semanas (flexible)
- ğŸ’° **Costo:** Incremental ($0 â†’ $30-50/mes)
- ğŸ“ˆ **Mejora esperada:** 70% â†’ 85% â†’ 92%+
- ğŸ¯ **Ideal para:** Minimizar riesgos, validar cada paso

**OpciÃ³n C: COMPLETA (MÃ¡ximo impacto)**
- âœ… **Alcance:** Todas las fases secuenciales (0-7)
- â±ï¸ **DuraciÃ³n:** 8-12 semanas continuas
- ğŸ’° **Costo:** $40-70/mes desde el inicio
- ğŸ“ˆ **Mejora esperada:** 70% â†’ 92%+ al final
- ğŸ¯ **Ideal para:** Si ya estÃ¡s seguro del ROI y quieres mÃ¡xima mejora

**ğŸ“ Tu respuesta:**
```
[ ] OpciÃ³n A - RÃPIDA (validar primero)
[ ] OpciÃ³n B - GRADUAL (recomendada)
[ ] OpciÃ³n C - COMPLETA (full implementation)
[ ] Otra: _________________________________
```

---

#### 2. Â¿CuÃ¡l es tu presupuesto mensual mÃ¡ximo?

Este presupuesto define quÃ© tipo de proxies y servicios podemos usar:

**OpciÃ³n A: $0 (Solo soluciones gratuitas)**
- âœ… Proxies: Webshare (10 gratis) + Free proxy lists (GitHub)
- âœ… CAPTCHA: Manual o sin resolver
- âœ… Framework: Crawlee + rebrowser-patches (gratis)
- âš ï¸ **Limitaciones:**
  - Success rate limitado (~75-80%)
  - Proxies no confiables
  - No apto para sitios con protecciÃ³n fuerte
- ğŸ¯ **Ideal para:** Testing, proyectos personales, POC

**OpciÃ³n B: $20-30/mes (Budget consciente)**
- âœ… Proxies: ProxyWing ($2.50/GB residential, ~10GB)
- âœ… CAPTCHA: NoCaptchaAI ($10 = 71,428 CAPTCHAs)
- âœ… Framework: Crawlee + rebrowser-patches (gratis)
- âœ… **Capacidad:** ~50,000 requests/mes
- ğŸ“ˆ **Success rate esperado:** 85-90%
- ğŸ¯ **Ideal para:** Startups, freelancers, MVPs

**OpciÃ³n C: $50-100/mes (ProducciÃ³n)**
- âœ… Proxies: Smartproxy ($3.50/GB, ~20-30GB) o ScraperAPI ($49/mes)
- âœ… CAPTCHA: 2Captcha ($20-30)
- âœ… Framework: Crawlee + rebrowser-patches + fallback a API
- âœ… **Capacidad:** 500K-2M requests/mes
- ğŸ“ˆ **Success rate esperado:** 90-95%
- ğŸ¯ **Ideal para:** ProducciÃ³n, negocios establecidos

**OpciÃ³n D: $100+/mes (Sin lÃ­mite)**
- âœ… Proxies: Bright Data, Oxylabs (premium)
- âœ… CAPTCHA: Servicios premium
- âœ… Framework: ScraperAPI Enterprise + custom solutions
- âœ… **Capacidad:** 10M+ requests/mes
- ğŸ“ˆ **Success rate esperado:** 95-99%
- ğŸ¯ **Ideal para:** Enterprise, alto volumen

**ğŸ“ Tu respuesta:**
```
Presupuesto mensual mÃ¡ximo: $__________

Desglose preferido:
- Proxies: $__________
- CAPTCHA: $__________
- Otros: $__________
```

---

#### 3. Â¿QuÃ© fases quieres aprobar para implementaciÃ³n?

Marca con âœ… las fases que autorizas. Puedes aprobar todas o solo algunas:

```
[ ] FASE 0: PreparaciÃ³n (4-6h)
    - Backup, instalaciÃ³n de dependencias, configuraciÃ³n
    - Riesgo: ğŸŸ¢ Muy bajo | Costo: $0

[ ] FASE 1: Quick Wins (5-6h) â­ ALTA PRIORIDAD
    - User-Agents 2025, Sec-CH-UA headers, rebrowser, rate limiting
    - Riesgo: ğŸŸ¢ Muy bajo | Costo: $0 | Impacto: +10-15% inmediato

[ ] FASE 2: MÃ³dulo ScrapingModule Base (12-16h)
    - Servicios de proxies, fingerprints, mÃ©tricas
    - Riesgo: ğŸŸ¡ Medio | Costo: $0

[ ] FASE 3: Anti-DetecciÃ³n Avanzada (12-18h)
    - AdvancedScrapingService, fallback chain (Cheerioâ†’Rebrowserâ†’Crawlee)
    - Riesgo: ğŸŸ¡ Medio | Costo: $0

[ ] FASE 4: CAPTCHA & MÃ©tricas (8-12h)
    - CaptchaSolverService, dashboard de mÃ©tricas
    - Riesgo: ğŸŸ¢ Bajo | Costo: $10-20/mes

[ ] FASE 5: IntegraciÃ³n con MÃ³dulos Existentes (8-10h) âš ï¸ CRÃTICA
    - Modificar NoticiasModule y GeneratorProModule
    - Riesgo: ğŸ”´ Alto | Costo: $0
    - âš ï¸ Requiere aprobaciÃ³n EXPLÃCITA (cambia cÃ³digo core)

[ ] FASE 6: Testing Completo (12-16h)
    - Testing extensivo, configuraciÃ³n de proxies reales
    - Riesgo: ğŸŸ¡ Medio | Costo: $30-50/mes

[ ] FASE 7: Deploy a ProducciÃ³n (4-6h)
    - Staging â†’ ProducciÃ³n con monitoring
    - Riesgo: ğŸŸ¡ Medio | Costo: $40-70/mes
```

---

#### 4. Â¿QuÃ© nivel de mejora esperas/necesitas?

Define tus expectativas de success rate:

**Escenario A: Mejora BÃ¡sica**
- ğŸ“Š **Objetivo:** 70% â†’ 80-85%
- âœ… **Fases necesarias:** 0, 1, 2, 3
- â±ï¸ **Tiempo:** ~4-6 semanas
- ğŸ’° **Costo:** $0-20/mes

**Escenario B: Mejora Significativa (Recomendado)**
- ğŸ“Š **Objetivo:** 70% â†’ 90-92%
- âœ… **Fases necesarias:** 0, 1, 2, 3, 4, 5, 6
- â±ï¸ **Tiempo:** ~8-10 semanas
- ğŸ’° **Costo:** $30-50/mes

**Escenario C: MÃ¡xima OptimizaciÃ³n**
- ğŸ“Š **Objetivo:** 70% â†’ 95%+
- âœ… **Fases necesarias:** Todas (0-7) + optimizaciones continuas
- â±ï¸ **Tiempo:** ~10-12 semanas + ongoing
- ğŸ’° **Costo:** $50-100/mes

**ğŸ“ Tu respuesta:**
```
Success rate objetivo: ______%
Tiempo disponible: ______ semanas
```

---

#### 5. Â¿CuÃ¡ndo quieres empezar?

**OpciÃ³n A: Ahora (inmediatamente)**
- Empezamos con Fase 0 en cuanto apruebes
- Timeline agresivo

**OpciÃ³n B: Fecha especÃ­fica**
- Inicio planificado: ___/___/2025
- Permite preparaciÃ³n y coordinaciÃ³n

**OpciÃ³n C: MÃ¡s adelante**
- Plan queda documentado para implementaciÃ³n futura
- Sin fecha definida aÃºn

**ğŸ“ Tu respuesta:**
```
Fecha de inicio preferida: ___/___/2025
```

---

#### 6. Â¿QuiÃ©n harÃ¡ el testing manual?

Como mencionaste que NO haremos testing automÃ¡tico:

**ğŸ“ Tu respuesta:**
```
Persona responsable de testing: _______________________
Disponibilidad para testing: _______ horas/semana
Horario preferido: _______________________
```

---

#### 7. Â¿Tienes alguna restricciÃ³n o requerimiento especial?

Marca todas las que apliquen:

```
[ ] No podemos tener downtime en producciÃ³n
[ ] Necesitamos mantener el cÃ³digo 100% compatible con versiÃ³n actual
[ ] Tenemos restricciones de presupuesto muy estrictas
[ ] Necesitamos documentaciÃ³n exhaustiva de cada cambio
[ ] Queremos poder hacer rollback fÃ¡cilmente
[ ] Tenemos ventanas de deploy especÃ­ficas (especificar: _______)
[ ] Necesitamos aprobar cada fase individualmente
[ ] Otros: _________________________________
```

---

### ğŸ“‹ Resumen de Decisiones

Una vez respondidas las preguntas, completa este resumen:

```
=================================================
CONFIGURACIÃ“N DEL PROYECTO - ANTI-DETECCIÃ“N SYSTEM
=================================================

Estrategia: [ ] RÃ¡pida  [ ] Gradual  [ ] Completa
Presupuesto mensual: $__________
Fases aprobadas: ___________________________
Success rate objetivo: ______%
Fecha de inicio: ___/___/2025
Responsable de testing: _____________________

Notas adicionales:
_________________________________________________
_________________________________________________
_________________________________________________

Aprobado por: ________________  Fecha: ___/___/2025
```

---

## VisiÃ³n General

### Objetivo Principal
Implementar un sistema avanzado de scraping con anti-detecciÃ³n que mejore la tasa de Ã©xito del 70% actual a 92%+ mediante:
- RotaciÃ³n inteligente de proxies (tiered escalation)
- TecnologÃ­a anti-detecciÃ³n moderna (rebrowser-patches)
- ResoluciÃ³n de CAPTCHAs
- GestiÃ³n de rate limiting avanzada
- Sistema de mÃ©tricas y monitoreo

### FilosofÃ­a de ImplementaciÃ³n
âœ… **SIN forwardRef** - Usar EventEmitter2 para comunicaciÃ³n entre mÃ³dulos
âœ… **SIN any types** - Tipado estricto TypeScript
âœ… **SIN testing automÃ¡tico** - Testing manual por el equipo
âœ… **Build despuÃ©s de cada fase** - Verificar que el servidor levanta sin errores
âœ… **ImplementaciÃ³n incremental** - Cada fase es funcional e independiente

### Presupuesto Estimado
- **Desarrollo:** 0 horas (soluciÃ³n open-source)
- **Infraestructura:** $30-50/mes
  - ProxyWing: $25/mes (10GB residential)
  - NoCaptchaAI: $10/mes
  - VPS actual: $0 (ya existente)

---

## AnÃ¡lisis del Sistema Actual

### Flujo de ExtracciÃ³n Actual

#### 1. ExtracciÃ³n de URLs (MÃ³dulo Generator-Pro)
```
RapidAPI Facebook â†’ UrlDetectionService
                        â†“
            ExtracciÃ³n de URLs de posts
                        â†“
            Almacenamiento en ExternalUrl
                        â†“
            UrlExtractionService.extractUrls()
                        â†“
            Scraping de listing pages (Cheerio/Puppeteer)
                        â†“
            Parsing con CSS selectors (listingSelectors.articleLinks)
                        â†“
            Hash SHA-256 para deduplicaciÃ³n
                        â†“
            VerificaciÃ³n en ExtractedUrlTracking
                        â†“
            Queue en Bull (extract_content)
```

#### 2. ExtracciÃ³n de Contenido
```
Bull Queue â†’ NoticiasExtractionProcessor / ExtractionProcessor
                        â†“
            Cache check (Redis, 30 min TTL)
                        â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Cache HIT?        â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ YES                       â”‚ NO
        â”‚                           â”‚
        â–¼                           â–¼
    Return cached            Scraping Execution
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ useJavaScript?                â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ NO                    â”‚ YES
                â”‚                       â”‚
                â–¼                       â–¼
        Cheerio (Static)        Puppeteer (Dynamic)
        - axios.get()           - PuppeteerManagerService
        - cheerio.load()        - Browser pool (max 5)
        - CSS selectors         - Stealth config
                                - Request interception
                â”‚                       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                    ExtracciÃ³n de datos
                    - title, content, images
                    - date, author, categories
                    - keywords (frequency analysis)
                            â–¼
                    Data cleaning
                    - Remove scripts, styles
                    - Normalize whitespace
                    - Extract keywords
                            â–¼
                    Quality metrics
                    - Title/content quality
                    - Completeness (0-100%)
                    - Confidence score
                            â–¼
                    Almacenamiento
                    - ExtractedNoticia (MongoDB)
                    - NoticiasExtractionLog
                    - Update URL status
```

### Servicios Involucrados

#### MÃ³dulo Noticias
```typescript
// packages/api-nueva/src/noticias/

NoticiasScrapingService
â”œâ”€ extractFromUrl(url, config)
â”œâ”€ extractWithCheerio(url, config)
â”œâ”€ extractWithPuppeteer(url, config)
â”œâ”€ enforceRateLimit(domain)
â”œâ”€ getRandomUserAgent()
â””â”€ parseHtml(html, selectors)

NoticiasExtractionService
â”œâ”€ queueExtraction(configId, urls)
â”œâ”€ processBatch(urls)
â””â”€ retryFailed(jobId)

NoticiasConfigService
â”œâ”€ create(config)
â”œâ”€ update(id, config)
â””â”€ testSelectors(url, selectors)

UrlDetectionService
â””â”€ findExternalUrls(facebookPosts)

NoticiasEventsService (EventEmitter2)
â””â”€ emitExtractionComplete(data)
```

#### MÃ³dulo Generator-Pro
```typescript
// packages/api-nueva/src/generator-pro/

NewsWebsiteService
â”œâ”€ extractNewsContent(url, configId)
â”œâ”€ scrapeWithCheerio(url)
â””â”€ scrapeWithPuppeteer(url)

UrlExtractionService
â”œâ”€ extractUrls(websiteConfigId)
â”œâ”€ processExtractedUrls(urls)
â””â”€ deduplicateUrls(urls)

GeneratorProOrchestratorService
â”œâ”€ runFullPipeline(websiteConfigId)
â”œâ”€ orchestrateExtraction()
â”œâ”€ orchestrateGeneration()
â””â”€ orchestratePublishing()
```

#### MÃ³dulo Reports (Compartido)
```typescript
// packages/api-nueva/src/modules/reports/

PuppeteerManagerService
â”œâ”€ launchBrowser()
â”œâ”€ getRenderedHTML(url, options)
â”œâ”€ healthCheck()
â”œâ”€ cleanup()
â””â”€ metrics tracking (uptime, errors)
```

### TecnologÃ­as Actuales
- **HTTP Client:** axios v1.12.2
- **HTML Parser:** cheerio v1.1.2
- **Browser:** puppeteer v24.25.0 + puppeteer-extra + stealth plugin
- **Queue:** bull v4.16.5 (Redis backend)
- **Cache:** @nestjs/cache-manager v3+ (@keyv/redis)
- **Events:** @nestjs/event-emitter v3.0.1

### Capacidades Anti-DetecciÃ³n Actuales
âœ… User-Agent rotation (5 UAs - **versiones 2024**)
âœ… Rate limiting (domain-specific, Redis-backed)
âœ… Cache strategy (SHA-256 keys, 30 min TTL)
âœ… Custom headers (realistic pero **sin Sec-CH-UA**)
âœ… Puppeteer optimization flags
âœ… Request interception (bloquea imÃ¡genes/CSS/fonts)

âŒ **NO hay rotaciÃ³n de proxies**
âŒ **NO hay rebrowser-patches** (Cloudflare bypass)
âŒ **NO hay resoluciÃ³n de CAPTCHAs**
âŒ **NO hay fingerprint moderno** (2025)
âŒ **NO hay TLS fingerprinting fixes**
âŒ **NO hay mÃ©tricas de Ã©xito por dominio**

### Problemas Identificados
1. **Bloqueos por IP** - Sin proxies, requests desde misma IP
2. **Cloudflare challenges** - Stealth plugin no funciona en 2025
3. **User-Agents desactualizados** - Chrome 120 â†’ debe ser Chrome 131
4. **Headers incompletos** - Falta Sec-CH-UA (requerido 2025)
5. **Sin fallback strategy** - Si Cheerio/Puppeteer fallan, no hay plan B
6. **Sin mÃ©tricas** - No se sabe quÃ© dominios bloquean mÃ¡s

---

## Arquitectura Propuesta

### Nuevo MÃ³dulo: ScrapingModule

**UbicaciÃ³n:** `/packages/api-nueva/src/scraping/`

**Estructura:**
```
src/scraping/
â”œâ”€â”€ scraping.module.ts
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ advanced-scraping.service.ts       # Core: LÃ³gica principal con fallback chain
â”‚   â”œâ”€â”€ proxy-configuration.service.ts     # GestiÃ³n de proxies (tiered escalation)
â”‚   â”œâ”€â”€ fingerprint-generator.service.ts   # GeneraciÃ³n de fingerprints modernos
â”‚   â”œâ”€â”€ captcha-solver.service.ts          # IntegraciÃ³n NoCaptchaAI / 2Captcha
â”‚   â”œâ”€â”€ rate-limiter.service.ts            # Bottleneck + exponential backoff
â”‚   â””â”€â”€ scraping-metrics.service.ts        # MÃ©tricas de Ã©xito por dominio
â”œâ”€â”€ dto/
â”‚   â”œâ”€â”€ scraper-config.dto.ts              # DTOs con validaciÃ³n
â”‚   â””â”€â”€ scraping-result.dto.ts
â”œâ”€â”€ interfaces/
â”‚   â”œâ”€â”€ proxy.interface.ts
â”‚   â”œâ”€â”€ fingerprint.interface.ts
â”‚   â””â”€â”€ scraping.interface.ts
â””â”€â”€ utils/
    â”œâ”€â”€ user-agents.util.ts                # UAs 2025 (Chrome 131, Firefox 133)
    â””â”€â”€ headers.util.ts                    # Headers con Sec-CH-UA
```

### Principios de DiseÃ±o

#### 1. CentralizaciÃ³n sin Circular Dependencies
```typescript
// âŒ MAL (circular)
NoticiasModule â†’ ScrapingModule â†’ NoticiasModule

// âœ… BIEN (unidireccional)
ScrapingModule (independiente)
    â†‘
    â”‚ usa
    â”‚
    â”œâ”€ NoticiasModule
    â””â”€ GeneratorProModule
```

#### 2. ComunicaciÃ³n con EventEmitter2
```typescript
// Sin forwardRef, comunicaciÃ³n por eventos
@Injectable()
export class ScrapingMetricsService {
  constructor(private eventEmitter: EventEmitter2) {}

  recordSuccess(url: string, data: any) {
    this.eventEmitter.emit('scraping.success', { url, ...data });
  }
}

// Otros mÃ³dulos escuchan
@Injectable()
export class NoticiasEventsService {
  @OnEvent('scraping.success')
  handleScrapingSuccess(payload: any) {
    // Actualizar estadÃ­sticas
  }
}
```

#### 3. Tipado Estricto (Sin Any)
```typescript
// âŒ MAL
async scrape(url: string, config: any): Promise<any>

// âœ… BIEN
async scrape(url: string, config: ScraperConfig): Promise<ScrapingResult>

interface ScraperConfig {
  selectors: SelectorConfig;
  useProxy: boolean;
  useJavaScript: boolean;
  timeout: number;
  proxyOptions?: ProxyOptions;
}

interface ScrapingResult {
  success: boolean;
  data: ExtractedContent;
  metadata: ScrapingMetadata;
  method: 'cheerio' | 'rebrowser' | 'crawlee' | 'scraperapi';
  cost: number;
}
```

### Flujo Mejorado con Anti-DetecciÃ³n

```
URL a scraper
      â†“
RateLimiterService.checkAndWait(url)
      â†“
ProxyConfigurationService.getProxy(url)
      â”œâ”€ Consulta mÃ©tricas del dominio
      â”œâ”€ Si dominio bloqueado por Cloudflare â†’ proxy residential/mobile
      â”œâ”€ Si dominio sin problemas â†’ sin proxy o datacenter
      â””â”€ Retorna proxy con health check
      â†“
FingerprintGeneratorService.generate()
      â”œâ”€ Genera UA moderno (Chrome 131, Firefox 133)
      â”œâ”€ Genera Sec-CH-UA headers consistentes
      â”œâ”€ Genera viewport, screen resolution
      â””â”€ Retorna fingerprint completo
      â†“
AdvancedScrapingService.scrape(url, config, proxy, fingerprint)
      â†“
Fallback Chain (intentar hasta que funcione):
      â”œâ”€ 1. Cheerio (rÃ¡pido, barato, sin JS) â†’ Si falla...
      â”œâ”€ 2. Rebrowser-Puppeteer (anti-detecciÃ³n, bypasa Cloudflare) â†’ Si falla...
      â”œâ”€ 3. Crawlee (mÃ¡xima automatizaciÃ³n) â†’ Si falla...
      â””â”€ 4. ScraperAPI (garantizado, caro) â†’ Si falla â†’ Error
      â†“
Si CAPTCHA detectado:
      â”œâ”€ CaptchaSolverService.solveCaptcha(page, url)
      â”œâ”€ NoCaptchaAI / 2Captcha
      â””â”€ Retry navegaciÃ³n
      â†“
ExtracciÃ³n exitosa
      â†“
ScrapingMetricsService.recordAttempt(url, result)
      â”œâ”€ Incrementa success/failure counters
      â”œâ”€ Actualiza proxy metrics
      â”œâ”€ Emite evento 'scraping.success' o 'scraping.failure'
      â””â”€ Trigger alertas si tasa de fallo > 50%
      â†“
Return ScrapingResult
```

### IntegraciÃ³n con MÃ³dulos Existentes

#### NoticiasScrapingService (MODIFICAR)
```typescript
@Injectable()
export class NoticiasScrapingService {
  constructor(
    // Existentes
    private puppeteerManager: PuppeteerManagerService,
    private cacheService: CacheService,
    // NUEVO
    private advancedScrapingService: AdvancedScrapingService,
  ) {}

  async extractFromUrl(url: string, config: NoticiasExtractionConfig) {
    // Delegar al nuevo servicio
    return this.advancedScrapingService.scrape(url, {
      selectors: config.selectors,
      useProxy: config.extractionSettings.useProxy ?? true,
      useJavaScript: config.extractionSettings.useJavaScript ?? false,
      timeout: config.extractionSettings.timeout ?? 30000,
    });
  }
}
```

#### NewsWebsiteService (MODIFICAR)
```typescript
@Injectable()
export class NewsWebsiteService {
  constructor(
    // NUEVO
    private advancedScrapingService: AdvancedScrapingService,
  ) {}

  async extractNewsContent(url: string, configId: string) {
    const config = await this.websiteConfigModel.findById(configId);

    return this.advancedScrapingService.scrape(url, {
      selectors: config.contentSelectors,
      useProxy: true, // Siempre usar proxy en generator-pro
      useJavaScript: config.extractionSettings?.useJavaScript ?? false,
    });
  }
}
```

---

## Plan de ImplementaciÃ³n por Fases

### Fase 0: PreparaciÃ³n (1 dÃ­a)
**DuraciÃ³n:** 4-6 horas
**Build requerido:** SÃ­

**Objetivo:** Preparar el entorno y dependencias sin romper nada

#### Tareas:
1. Backup de base de datos
2. Crear branch feature/advanced-anti-detection
3. Instalar dependencias nuevas
4. Configurar variables de entorno
5. Verificar build exitoso

---

### Fase 1: Quick Wins (1 dÃ­a)
**DuraciÃ³n:** 5-6 horas
**Build requerido:** SÃ­
**Impacto esperado:** +10-15% tasa de Ã©xito inmediata

**Objetivo:** Mejoras rÃ¡pidas sin cambios arquitectÃ³nicos

#### Tareas:
1. Actualizar User-Agents a 2025
2. Agregar headers Sec-CH-UA
3. Instalar rebrowser-puppeteer-core
4. Instalar Bottleneck para rate limiting
5. Testing manual

---

### Fase 2: MÃ³dulo ScrapingModule Base (2-3 dÃ­as)
**DuraciÃ³n:** 12-16 horas
**Build requerido:** SÃ­ (despuÃ©s de cada servicio)

**Objetivo:** Crear infraestructura central sin romper mÃ³dulos existentes

#### Tareas:
1. Crear estructura de carpetas
2. Crear interfaces y DTOs
3. Implementar AdvancedScrapingService (solo estructura)
4. Implementar ProxyConfigurationService
5. Implementar FingerprintGeneratorService
6. Implementar RateLimiterService
7. Testing manual de cada servicio

---

### Fase 3: Anti-DetecciÃ³n Avanzada (2-3 dÃ­as)
**DuraciÃ³n:** 12-18 horas
**Build requerido:** SÃ­

**Objetivo:** Implementar lÃ³gica de scraping con rebrowser y Crawlee

#### Tareas:
1. Implementar scrapeWithRebrowser()
2. Implementar scrapeWithCrawlee()
3. Implementar fallback chain
4. Integrar fingerprints
5. Integrar proxies
6. Testing manual contra sitios protegidos

---

### Fase 4: CAPTCHA & MÃ©tricas (1-2 dÃ­as)
**DuraciÃ³n:** 8-12 horas
**Build requerido:** SÃ­

**Objetivo:** Completar funcionalidades avanzadas

#### Tareas:
1. Implementar CaptchaSolverService
2. Implementar ScrapingMetricsService
3. Integrar eventos con EventEmitter2
4. Dashboard de mÃ©tricas (API endpoint)
5. Testing manual

---

### Fase 5: IntegraciÃ³n con MÃ³dulos Existentes (1-2 dÃ­as)
**DuraciÃ³n:** 8-10 horas
**Build requerido:** SÃ­ (crÃ­tico)

**Objetivo:** Conectar nuevo sistema con NoticiasModule y GeneratorProModule

#### Tareas:
1. Modificar NoticiasScrapingService
2. Modificar NewsWebsiteService
3. Actualizar mÃ³dulos (imports)
4. Testing manual de flujo completo
5. Verificar backward compatibility

---

### Fase 6: Testing Completo & Ajustes (2-3 dÃ­as)
**DuraciÃ³n:** 12-16 horas
**Build requerido:** SÃ­

**Objetivo:** Validar que todo funciona en conjunto

#### Tareas:
1. Testing manual de noticias module
2. Testing manual de generator-pro module
3. Testing contra sitios reales con protecciÃ³n
4. Ajustar configuraciones de proxies
5. Optimizar rate limiting
6. Documentar cambios

---

### Fase 7: Deploy a ProducciÃ³n (1 dÃ­a)
**DuraciÃ³n:** 4-6 horas
**Build requerido:** SÃ­

**Objetivo:** Despliegue seguro sin downtime

#### Tareas:
1. Configurar proxies en producciÃ³n
2. Configurar API keys de CAPTCHA
3. Deploy a staging
4. Monitoring 24h en staging
5. Deploy a producciÃ³n (ventana de bajo trÃ¡fico)
6. Monitoring post-deploy

---

## Checklist Detallado

### FASE 0: PREPARACIÃ“N â±ï¸ 4-6 horas

#### 0.1 Backup y Seguridad
- [ ] **Backup de MongoDB** (producciÃ³n y desarrollo)
  ```bash
  mongodump --uri="mongodb://..." --out=backup-$(date +%Y%m%d)
  ```
- [ ] **Documentar estado actual de queues** (Redis)
  ```bash
  redis-cli KEYS "bull:*" > queue-snapshot.txt
  ```
- [ ] **Export de configuraciones** (NoticiasExtractionConfig, NewsWebsiteConfig)
  ```bash
  mongoexport --db=... --collection=noticiasextractionconfigs --out=configs-backup.json
  ```

#### 0.2 Git Workflow
- [ ] **Crear branch desde main**
  ```bash
  git checkout main
  git pull origin main
  git checkout -b feature/advanced-anti-detection
  ```
- [ ] **Proteger branch main** (GitHub settings)

#### 0.3 InstalaciÃ³n de Dependencias
- [ ] **Instalar rebrowser-puppeteer-core**
  ```bash
  cd packages/api-nueva
  npm install rebrowser-puppeteer-core --save
  ```
- [ ] **Instalar Crawlee + Playwright**
  ```bash
  npm install crawlee playwright --save
  ```
- [ ] **Instalar proxy agents**
  ```bash
  npm install https-proxy-agent socks-proxy-agent --save
  ```
- [ ] **Instalar Bottleneck**
  ```bash
  npm install bottleneck --save
  npm install @types/bottleneck --save-dev
  ```
- [ ] **Instalar 2captcha (opcional para Fase 4)**
  ```bash
  npm install 2captcha --save
  ```

#### 0.4 Variables de Entorno
- [ ] **Actualizar .env.example**
  ```bash
  # Anti-Detection Configuration
  DATACENTER_PROXY_URLS=
  RESIDENTIAL_PROXY_URLS=
  MOBILE_PROXY_URLS=

  # CAPTCHA Services
  NOCAPTCHAAI_API_KEY=
  TWOCAPTCHA_API_KEY=

  # ScraperAPI (fallback)
  SCRAPERAPI_KEY=
  SCRAPERAPI_ENABLED=false

  # Puppeteer
  MAX_CONCURRENT_BROWSERS=5
  BROWSER_POOL_SIZE=10

  # Rate Limiting
  DEFAULT_RATE_LIMIT=30
  ADAPTIVE_RATE_LIMITING=true

  # Monitoring
  ENABLE_SCRAPING_METRICS=true
  ```

- [ ] **Copiar a .env local**
  ```bash
  cp .env.example .env
  # Editar .env con valores reales (proxies en Fase 3)
  ```

#### 0.5 VerificaciÃ³n de Build
- [ ] **Build del proyecto**
  ```bash
  cd packages/api-nueva
  yarn build
  ```
- [ ] **Verificar output** - Sin errores de TypeScript
- [ ] **Verificar que servidor levanta** (NO ejecutar, solo verificar)
  ```bash
  # NO ejecutar: yarn start:dev
  # Solo verificar que build pasÃ³
  ```

---

### FASE 1: QUICK WINS â±ï¸ 5-6 horas

#### 1.1 Actualizar User-Agents (30 min)
- [ ] **Crear archivo de utilidad**
  ```bash
  mkdir -p src/scraping/utils
  touch src/scraping/utils/user-agents.util.ts
  ```

- [ ] **Implementar user-agents modernos**
  ```typescript
  // src/scraping/utils/user-agents.util.ts
  export const MODERN_USER_AGENTS_2025 = [
    // Chrome 131 (Diciembre 2025)
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',

    // Firefox 133 (Diciembre 2025)
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:133.0) Gecko/20100101 Firefox/133.0',
    'Mozilla/5.0 (X11; Linux x86_64; rv:133.0) Gecko/20100101 Firefox/133.0',
  ];

  export function getRandomUserAgent(): string {
    return MODERN_USER_AGENTS_2025[
      Math.floor(Math.random() * MODERN_USER_AGENTS_2025.length)
    ];
  }
  ```

- [ ] **Actualizar NoticiasScrapingService**
  ```typescript
  // packages/api-nueva/src/noticias/services/noticias-scraping.service.ts
  import { MODERN_USER_AGENTS_2025, getRandomUserAgent } from '../../scraping/utils/user-agents.util';

  // Reemplazar userAgents antiguos:
  private readonly userAgents = MODERN_USER_AGENTS_2025;

  // O usar funciÃ³n:
  private getRandomUserAgent() {
    return getRandomUserAgent();
  }
  ```

- [ ] **Actualizar NewsWebsiteService** (mismo cambio)

#### 1.2 Agregar Headers Sec-CH-UA (1 hora)
- [ ] **Crear archivo de utilidad**
  ```bash
  touch src/scraping/utils/headers.util.ts
  ```

- [ ] **Implementar generador de headers**
  ```typescript
  // src/scraping/utils/headers.util.ts
  export function generateModernHeaders(userAgent: string): Record<string, string> {
    const isChrome = userAgent.includes('Chrome');
    const isFirefox = userAgent.includes('Firefox');

    const baseHeaders = {
      'User-Agent': userAgent,
      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
      'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
      'Accept-Encoding': 'gzip, deflate, br',
      'Connection': 'keep-alive',
      'Upgrade-Insecure-Requests': '1',
    };

    if (isChrome) {
      return {
        ...baseHeaders,
        'sec-ch-ua': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'none',
        'sec-fetch-user': '?1',
      };
    }

    return baseHeaders;
  }
  ```

- [ ] **Actualizar NoticiasScrapingService** (mÃ©todo extractWithCheerio)
  ```typescript
  import { generateModernHeaders } from '../../scraping/utils/headers.util';

  const userAgent = this.getRandomUserAgent();
  const headers = generateModernHeaders(userAgent);

  const response = await axios.get(url, {
    headers,
    timeout: config.extractionSettings.timeout || 30000,
  });
  ```

- [ ] **Actualizar PuppeteerManagerService**
  ```typescript
  import { generateModernHeaders } from '../../scraping/utils/headers.util';

  const userAgent = getRandomUserAgent();
  await page.setUserAgent(userAgent);
  await page.setExtraHTTPHeaders(generateModernHeaders(userAgent));
  ```

#### 1.3 Integrar Rebrowser-Puppeteer (2 horas)
- [ ] **Modificar PuppeteerManagerService**
  ```typescript
  // packages/api-nueva/src/modules/reports/services/puppeteer-manager.service.ts

  // Cambiar import
  // import puppeteer from 'puppeteer'; // âŒ VIEJO
  import puppeteer from 'rebrowser-puppeteer-core'; // âœ… NUEVO

  async launchBrowser() {
    // Configurar rebrowser patches
    process.env.REBROWSER_PATCHES_RUNTIME_FIX_MODE = 'addBinding';
    process.env.REBROWSER_PATCHES_SOURCE_URL = 'app.js';

    this.browser = await puppeteer.launch({
      headless: 'new',
      args: [
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-dev-shm-usage',
        '--disable-blink-features=AutomationControlled', // Ya existe
        // ... resto de args existentes
      ],
    });
  }
  ```

#### 1.4 Integrar Bottleneck Rate Limiting (2 horas)
- [ ] **Crear RateLimiterService bÃ¡sico**
  ```bash
  mkdir -p src/scraping/services
  touch src/scraping/services/rate-limiter.service.ts
  ```

- [ ] **Implementar servicio**
  ```typescript
  // src/scraping/services/rate-limiter.service.ts
  import { Injectable } from '@nestjs/common';
  import Bottleneck from 'bottleneck';

  @Injectable()
  export class RateLimiterService {
    private limiter: Bottleneck;

    constructor() {
      this.limiter = new Bottleneck({
        maxConcurrent: 5,      // Max 5 requests concurrentes
        minTime: 200,          // Min 200ms entre requests (5 req/sec)
      });
    }

    async schedule<T>(fn: () => Promise<T>): Promise<T> {
      return this.limiter.schedule(fn);
    }

    wrap<T extends (...args: any[]) => Promise<any>>(fn: T): T {
      return this.limiter.wrap(fn) as T;
    }
  }
  ```

- [ ] **Crear mÃ³dulo temporal**
  ```bash
  touch src/scraping/scraping.module.ts
  ```
  ```typescript
  // src/scraping/scraping.module.ts
  import { Module } from '@nestjs/common';
  import { RateLimiterService } from './services/rate-limiter.service';

  @Module({
    providers: [RateLimiterService],
    exports: [RateLimiterService],
  })
  export class ScrapingModule {}
  ```

- [ ] **Importar en AppModule**
  ```typescript
  // src/app.module.ts
  import { ScrapingModule } from './scraping/scraping.module';

  @Module({
    imports: [
      // ... existentes
      ScrapingModule, // NUEVO
    ],
  })
  export class AppModule {}
  ```

- [ ] **Usar en NoticiasScrapingService**
  ```typescript
  constructor(
    // ... existentes
    private rateLimiter: RateLimiterService, // NUEVO
  ) {}

  async extractFromUrl(url: string, config: any) {
    // Wrap con rate limiting
    return this.rateLimiter.schedule(async () => {
      // LÃ³gica existente de extracciÃ³n
    });
  }
  ```

#### 1.5 Testing Manual (30 min)
- [ ] **Build del proyecto**
  ```bash
  yarn build
  ```
- [ ] **Verificar sin errores de TypeScript**
- [ ] **Levantar servidor en modo dev** (Coyotito lo hace)
- [ ] **Probar extracciÃ³n de 1 URL de prueba**
- [ ] **Verificar headers en Network tab** (deben aparecer Sec-CH-UA)
- [ ] **Verificar logs** (debe usar UAs modernos)

#### 1.6 Commit de Fase 1
- [ ] **Commit cambios**
  ```bash
  git add .
  git commit -m "feat: Phase 1 - Quick wins (modern UAs, Sec-CH-UA, rebrowser, rate limiting)"
  git push origin feature/advanced-anti-detection
  ```

---

### FASE 2: MÃ“DULO SCRAPINGMODULE BASE â±ï¸ 12-16 horas

#### 2.1 Estructura de Carpetas (15 min)
- [ ] **Crear estructura completa**
  ```bash
  mkdir -p src/scraping/services
  mkdir -p src/scraping/dto
  mkdir -p src/scraping/interfaces
  mkdir -p src/scraping/utils
  ```

#### 2.2 Interfaces y DTOs (1-2 horas)
- [ ] **Crear interfaces**
  ```bash
  touch src/scraping/interfaces/proxy.interface.ts
  touch src/scraping/interfaces/fingerprint.interface.ts
  touch src/scraping/interfaces/scraping.interface.ts
  ```

- [ ] **Implementar proxy.interface.ts**
  ```typescript
  export type ProxyType = 'datacenter' | 'residential' | 'mobile';
  export type ProxyTier = 'datacenter' | 'residential' | 'mobile';

  export interface Proxy {
    url: string;
    host: string;
    port: number;
    username?: string;
    password?: string;
    type: ProxyType;
  }

  export interface ProxyOptions {
    tier?: ProxyTier;
    forceRotation?: boolean;
  }

  export interface ProxyMetrics {
    totalRequests: number;
    successfulRequests: number;
    failedRequests: number;
    failureRate: number;
    avgResponseTime: number;
    lastUsed: Date;
  }
  ```

- [ ] **Implementar fingerprint.interface.ts**
  ```typescript
  export interface Fingerprint {
    userAgent: string;
    secChUa: string;
    secChUaMobile: string;
    secChUaPlatform: string;
    platform: string;
    viewport: {
      width: number;
      height: number;
    };
    screen: {
      width: number;
      height: number;
      colorDepth: number;
    };
    timezone: string;
    languages: string[];
    hardwareConcurrency: number;
  }
  ```

- [ ] **Implementar scraping.interface.ts**
  ```typescript
  export interface ScraperConfig {
    selectors: SelectorConfig;
    useProxy: boolean;
    useJavaScript: boolean;
    timeout: number;
    proxyOptions?: ProxyOptions;
    retryAttempts?: number;
  }

  export interface SelectorConfig {
    title: string;
    content: string;
    images?: string | string[];
    publishedAt?: string;
    author?: string;
    categories?: string | string[];
    tags?: string | string[];
  }

  export interface ScrapingResult {
    success: boolean;
    data: ExtractedContent;
    metadata: ScrapingMetadata;
    method: 'cheerio' | 'rebrowser' | 'crawlee' | 'scraperapi';
    cost: number;
  }

  export interface ExtractedContent {
    title: string;
    content: string;
    images: string[];
    publishedAt?: Date;
    author?: string;
    categories?: string[];
    tags?: string[];
    keywords?: string[];
  }

  export interface ScrapingMetadata {
    url: string;
    processingTime: number;
    proxyUsed?: string;
    proxyType?: ProxyType;
    captchaEncountered: boolean;
    captchaSolved: boolean;
    blockedByCloudflare: boolean;
    blockedByDataDome: boolean;
    httpStatus: number;
    contentLength: number;
  }
  ```

- [ ] **Crear DTOs con validaciÃ³n**
  ```bash
  touch src/scraping/dto/scraper-config.dto.ts
  touch src/scraping/dto/scraping-result.dto.ts
  ```

- [ ] **Implementar DTOs** (con class-validator)
  ```typescript
  // scraper-config.dto.ts
  import { IsBoolean, IsNumber, IsObject, IsOptional, Min, Max } from 'class-validator';

  export class ScraperConfigDto {
    @IsObject()
    selectors: any; // Usar SelectorConfig en producciÃ³n

    @IsBoolean()
    @IsOptional()
    useProxy?: boolean = true;

    @IsBoolean()
    @IsOptional()
    useJavaScript?: boolean = false;

    @IsNumber()
    @Min(1000)
    @Max(120000)
    @IsOptional()
    timeout?: number = 30000;
  }
  ```

#### 2.3 ProxyConfigurationService (3-4 horas)
- [ ] **Crear servicio**
  ```bash
  touch src/scraping/services/proxy-configuration.service.ts
  ```

- [ ] **Implementar lÃ³gica completa**
  - [ ] loadProxies() - Leer de ENV y parsear
  - [ ] getProxy() - Seleccionar proxy basado en mÃ©tricas
  - [ ] determineRequiredTier() - EscalaciÃ³n inteligente
  - [ ] selectBestProxy() - Round-robin con health check
  - [ ] trackProxyUsage() - Registrar uso
  - [ ] updateProxyMetrics() - Actualizar mÃ©tricas post-request

- [ ] **Ver implementaciÃ³n completa en rotation_scraper_for_escape_blockers.md lÃ­neas 522-638**

- [ ] **Testing manual**
  ```typescript
  // Crear test temporal en controller
  @Get('test/proxy')
  async testProxy() {
    const proxy = await this.proxyService.getProxy('https://example.com');
    return proxy;
  }
  ```

#### 2.4 FingerprintGeneratorService (2-3 horas)
- [ ] **Crear servicio**
  ```bash
  touch src/scraping/services/fingerprint-generator.service.ts
  ```

- [ ] **Implementar mÃ©todos**
  ```typescript
  @Injectable()
  export class FingerprintGeneratorService {
    generate(): Fingerprint {
      const userAgent = getRandomUserAgent();
      const isChrome = userAgent.includes('Chrome');

      return {
        userAgent,
        secChUa: isChrome ? '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"' : '',
        secChUaMobile: '?0',
        secChUaPlatform: '"Windows"',
        platform: 'Win32',
        viewport: this.generateViewport(),
        screen: this.generateScreen(),
        timezone: 'America/Mexico_City',
        languages: ['es-MX', 'es', 'en-US', 'en'],
        hardwareConcurrency: 8,
      };
    }

    private generateViewport() {
      const common = [
        { width: 1920, height: 1080 },
        { width: 1366, height: 768 },
        { width: 1440, height: 900 },
      ];
      return common[Math.floor(Math.random() * common.length)];
    }

    private generateScreen() {
      const viewport = this.generateViewport();
      return {
        width: viewport.width,
        height: viewport.height,
        colorDepth: 24,
      };
    }

    async applyToPage(page: any, fingerprint: Fingerprint): Promise<void> {
      await page.setViewport(fingerprint.viewport);

      await page.evaluateOnNewDocument((fp) => {
        Object.defineProperty(navigator, 'platform', { get: () => fp.platform });
        Object.defineProperty(navigator, 'languages', { get: () => fp.languages });
        Object.defineProperty(navigator, 'hardwareConcurrency', { get: () => fp.hardwareConcurrency });
        Object.defineProperty(screen, 'width', { get: () => fp.screen.width });
        Object.defineProperty(screen, 'height', { get: () => fp.screen.height });
      }, fingerprint);
    }
  }
  ```

- [ ] **Testing manual** (con Puppeteer)

#### 2.5 ScrapingMetricsService (2-3 horas)
- [ ] **Crear servicio**
  ```bash
  touch src/scraping/services/scraping-metrics.service.ts
  ```

- [ ] **Implementar con EventEmitter2**
  ```typescript
  @Injectable()
  export class ScrapingMetricsService {
    private metrics = new Map<string, DomainMetrics>();

    constructor(private eventEmitter: EventEmitter2) {}

    async recordSuccess(url: string, metadata: ScrapingMetadata) {
      const domain = new URL(url).hostname;
      const domainMetrics = this.getOrCreateMetrics(domain);

      domainMetrics.totalRequests++;
      domainMetrics.successfulRequests++;
      domainMetrics.lastSuccessAt = new Date();

      this.eventEmitter.emit('scraping.success', { url, domain, metadata });
    }

    async recordFailure(url: string, error: string, metadata: Partial<ScrapingMetadata>) {
      const domain = new URL(url).hostname;
      const domainMetrics = this.getOrCreateMetrics(domain);

      domainMetrics.totalRequests++;
      domainMetrics.failedRequests++;

      if (metadata.blockedByCloudflare) domainMetrics.blockedByCloudflare++;
      if (metadata.blockedByDataDome) domainMetrics.blockedByDataDome++;
      if (metadata.captchaEncountered) domainMetrics.captchaEncountered++;

      this.eventEmitter.emit('scraping.failure', { url, domain, error, metadata });

      // Alertas
      if (this.getFailureRate(domain) > 0.5) {
        this.eventEmitter.emit('scraping.alert.high-failure-rate', { domain });
      }
    }

    getDomainMetrics(domain: string): DomainMetrics {
      return this.metrics.get(domain) || this.createDefaultMetrics();
    }

    getFailureRate(domain: string): number {
      const metrics = this.getDomainMetrics(domain);
      return metrics.totalRequests > 0
        ? metrics.failedRequests / metrics.totalRequests
        : 0;
    }
  }

  interface DomainMetrics {
    domain: string;
    totalRequests: number;
    successfulRequests: number;
    failedRequests: number;
    blockedByCloudflare: number;
    blockedByDataDome: number;
    captchaEncountered: number;
    avgResponseTime: number;
    lastSuccessAt: Date;
    lastFailureAt: Date;
  }
  ```

- [ ] **Crear endpoint de mÃ©tricas** (para dashboard)
  ```typescript
  @Get('scraping/metrics')
  async getMetrics() {
    return this.scrapingMetrics.getAllMetrics();
  }
  ```

#### 2.6 Actualizar ScrapingModule (30 min)
- [ ] **Registrar todos los servicios**
  ```typescript
  // src/scraping/scraping.module.ts
  import { Module } from '@nestjs/common';
  import { ConfigModule } from '@nestjs/config';
  import { CacheModule } from '@nestjs/cache-manager';

  import { RateLimiterService } from './services/rate-limiter.service';
  import { ProxyConfigurationService } from './services/proxy-configuration.service';
  import { FingerprintGeneratorService } from './services/fingerprint-generator.service';
  import { ScrapingMetricsService } from './services/scraping-metrics.service';

  @Module({
    imports: [ConfigModule, CacheModule],
    providers: [
      RateLimiterService,
      ProxyConfigurationService,
      FingerprintGeneratorService,
      ScrapingMetricsService,
    ],
    exports: [
      RateLimiterService,
      ProxyConfigurationService,
      FingerprintGeneratorService,
      ScrapingMetricsService,
    ],
  })
  export class ScrapingModule {}
  ```

#### 2.7 Testing y Build de Fase 2 (1 hora)
- [ ] **Build del proyecto**
  ```bash
  yarn build
  ```
- [ ] **Verificar sin errores**
- [ ] **Testing manual de cada servicio** (endpoints temporales)
- [ ] **Commit**
  ```bash
  git add .
  git commit -m "feat: Phase 2 - Base ScrapingModule (proxies, fingerprints, metrics)"
  git push origin feature/advanced-anti-detection
  ```

---

### FASE 3: ANTI-DETECCIÃ“N AVANZADA â±ï¸ 12-18 horas

#### 3.1 AdvancedScrapingService - Estructura (2 horas)
- [ ] **Crear servicio principal**
  ```bash
  touch src/scraping/services/advanced-scraping.service.ts
  ```

- [ ] **Implementar estructura base**
  ```typescript
  @Injectable()
  export class AdvancedScrapingService {
    private readonly logger = new Logger(AdvancedScrapingService.name);

    constructor(
      private proxyService: ProxyConfigurationService,
      private fingerprintService: FingerprintGeneratorService,
      private rateLimiter: RateLimiterService,
      private metricsService: ScrapingMetricsService,
      private cacheService: CacheService,
    ) {}

    async scrape(url: string, config: ScraperConfig): Promise<ScrapingResult> {
      const startTime = Date.now();

      try {
        // 1. Rate limiting
        await this.rateLimiter.schedule(async () => {});

        // 2. Proxy (si configurado)
        const proxy = config.useProxy
          ? await this.proxyService.getProxy(url, config.proxyOptions)
          : null;

        // 3. Fingerprint
        const fingerprint = await this.fingerprintService.generate();

        // 4. Fallback chain
        const result = await this.executeWithFallback(url, config, proxy, fingerprint);

        // 5. MÃ©tricas
        await this.metricsService.recordSuccess(url, result.metadata);

        return result;
      } catch (error) {
        await this.metricsService.recordFailure(url, error.message, {
          url,
          processingTime: Date.now() - startTime,
        } as any);
        throw error;
      }
    }

    private async executeWithFallback(
      url: string,
      config: ScraperConfig,
      proxy: Proxy | null,
      fingerprint: Fingerprint,
    ): Promise<ScrapingResult> {
      // A implementar en siguientes pasos
      throw new Error('Not implemented');
    }
  }
  ```

#### 3.2 MÃ©todo: scrapeWithCheerio (2 horas)
- [ ] **Implementar scraping estÃ¡tico**
  ```typescript
  private async scrapeWithCheerio(
    url: string,
    config: ScraperConfig,
    proxy: Proxy | null,
  ): Promise<ScrapingResult> {
    const startTime = Date.now();

    const axiosConfig: any = {
      timeout: config.timeout || 30000,
      headers: generateModernHeaders(getRandomUserAgent()),
    };

    if (proxy) {
      const { HttpsProxyAgent } = require('https-proxy-agent');
      axiosConfig.httpsAgent = new HttpsProxyAgent(proxy.url);
    }

    const response = await axios.get(url, axiosConfig);
    const html = response.data;

    const content = this.parseWithCheerio(html, config.selectors);

    return {
      success: true,
      data: content,
      metadata: {
        url,
        processingTime: Date.now() - startTime,
        proxyUsed: proxy?.url,
        proxyType: proxy?.type,
        captchaEncountered: false,
        captchaSolved: false,
        blockedByCloudflare: false,
        blockedByDataDome: false,
        httpStatus: response.status,
        contentLength: html.length,
      },
      method: 'cheerio',
      cost: 0,
    };
  }

  private parseWithCheerio(html: string, selectors: SelectorConfig): ExtractedContent {
    const $ = cheerio.load(html);

    return {
      title: $(selectors.title).first().text().trim(),
      content: $(selectors.content).text().trim(),
      images: this.extractImages($, selectors.images),
      publishedAt: selectors.publishedAt ? this.parseDate($(selectors.publishedAt).text()) : undefined,
      author: selectors.author ? $(selectors.author).text().trim() : undefined,
      categories: selectors.categories ? this.extractArray($, selectors.categories) : [],
      tags: selectors.tags ? this.extractArray($, selectors.tags) : [],
      keywords: this.extractKeywords($(selectors.content).text()),
    };
  }
  ```

#### 3.3 MÃ©todo: scrapeWithRebrowser (3-4 horas)
- [ ] **Implementar scraping con rebrowser-patches**
  ```typescript
  private async scrapeWithRebrowser(
    url: string,
    config: ScraperConfig,
    proxy: Proxy | null,
    fingerprint: Fingerprint,
  ): Promise<ScrapingResult> {
    const startTime = Date.now();

    // Configurar rebrowser patches
    process.env.REBROWSER_PATCHES_RUNTIME_FIX_MODE = 'addBinding';
    process.env.REBROWSER_PATCHES_SOURCE_URL = 'app.js';

    const puppeteer = require('rebrowser-puppeteer-core');

    const launchOptions: any = {
      headless: 'new',
      args: [
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-dev-shm-usage',
        '--disable-blink-features=AutomationControlled',
        '--disable-gpu',
      ],
    };

    if (proxy) {
      launchOptions.args.push(`--proxy-server=${proxy.url}`);
    }

    const browser = await puppeteer.launch(launchOptions);

    try {
      const page = await browser.newPage();

      // Aplicar fingerprint
      await this.fingerprintService.applyToPage(page, fingerprint);

      // Headers modernos
      await page.setExtraHTTPHeaders({
        'User-Agent': fingerprint.userAgent,
        'sec-ch-ua': fingerprint.secChUa,
        'sec-ch-ua-mobile': fingerprint.secChUaMobile,
        'sec-ch-ua-platform': fingerprint.secChUaPlatform,
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'accept-language': 'es-ES,es;q=0.9,en;q=0.8',
      });

      // Navegar
      await page.goto(url, {
        waitUntil: 'networkidle2',
        timeout: config.timeout || 30000,
      });

      // Delay humano
      await this.randomDelay(1000, 3000);

      // Extraer HTML
      const html = await page.content();
      const content = this.parseWithCheerio(html, config.selectors);

      return {
        success: true,
        data: content,
        metadata: {
          url,
          processingTime: Date.now() - startTime,
          proxyUsed: proxy?.url,
          proxyType: proxy?.type,
          captchaEncountered: false, // A detectar en Fase 4
          captchaSolved: false,
          blockedByCloudflare: false,
          blockedByDataDome: false,
          httpStatus: 200,
          contentLength: html.length,
        },
        method: 'rebrowser',
        cost: 0.001, // Estimado de costo de proxy
      };
    } finally {
      await browser.close();
    }
  }

  private async randomDelay(min: number, max: number): Promise<void> {
    const delay = Math.random() * (max - min) + min;
    return new Promise(resolve => setTimeout(resolve, delay));
  }
  ```

#### 3.4 MÃ©todo: scrapeWithCrawlee (3-4 horas)
- [ ] **Implementar scraping con Crawlee**
  ```typescript
  private async scrapeWithCrawlee(
    url: string,
    config: ScraperConfig,
    proxy: Proxy | null,
  ): Promise<ScrapingResult> {
    const { PlaywrightCrawler, ProxyConfiguration, Dataset } = require('crawlee');
    const startTime = Date.now();

    let extractedData: any = null;

    const proxyConfiguration = proxy
      ? new ProxyConfiguration({ proxyUrls: [proxy.url] })
      : undefined;

    const crawler = new PlaywrightCrawler({
      proxyConfiguration,
      maxConcurrency: 1,

      requestHandler: async ({ page }) => {
        await page.waitForLoadState('networkidle');
        const html = await page.content();
        extractedData = this.parseWithCheerio(html, config.selectors);
      },

      failedRequestHandler: async ({ request }, error) => {
        throw new Error(`Crawlee failed: ${error.message}`);
      },
    });

    await crawler.run([url]);

    if (!extractedData) {
      throw new Error('No data extracted');
    }

    return {
      success: true,
      data: extractedData,
      metadata: {
        url,
        processingTime: Date.now() - startTime,
        proxyUsed: proxy?.url,
        proxyType: proxy?.type,
        captchaEncountered: false,
        captchaSolved: false,
        blockedByCloudflare: false,
        blockedByDataDome: false,
        httpStatus: 200,
        contentLength: 0,
      },
      method: 'crawlee',
      cost: 0.001,
    };
  }
  ```

#### 3.5 Fallback Chain (1-2 horas)
- [ ] **Implementar lÃ³gica de fallback**
  ```typescript
  private async executeWithFallback(
    url: string,
    config: ScraperConfig,
    proxy: Proxy | null,
    fingerprint: Fingerprint,
  ): Promise<ScrapingResult> {
    const methods = [
      {
        name: 'cheerio',
        fn: () => this.scrapeWithCheerio(url, config, proxy),
        shouldTry: !config.useJavaScript,
      },
      {
        name: 'rebrowser',
        fn: () => this.scrapeWithRebrowser(url, config, proxy, fingerprint),
        shouldTry: true,
      },
      {
        name: 'crawlee',
        fn: () => this.scrapeWithCrawlee(url, config, proxy),
        shouldTry: true,
      },
    ];

    for (const method of methods) {
      if (!method.shouldTry) continue;

      try {
        this.logger.log(`Trying ${method.name} for ${url}`);
        const result = await method.fn();
        this.logger.log(`âœ… Success with ${method.name}`);
        return result;
      } catch (error) {
        this.logger.warn(`âŒ ${method.name} failed: ${error.message}`);

        // Si es el Ãºltimo mÃ©todo, throw error
        const isLast = methods.filter(m => m.shouldTry).indexOf(method) === methods.filter(m => m.shouldTry).length - 1;
        if (isLast) {
          throw error;
        }
      }
    }

    throw new Error('All scraping methods exhausted');
  }
  ```

#### 3.6 Registrar en MÃ³dulo (30 min)
- [ ] **Actualizar ScrapingModule**
  ```typescript
  import { AdvancedScrapingService } from './services/advanced-scraping.service';

  @Module({
    imports: [ConfigModule, CacheModule],
    providers: [
      // ... existentes
      AdvancedScrapingService, // NUEVO
    ],
    exports: [
      // ... existentes
      AdvancedScrapingService, // NUEVO
    ],
  })
  export class ScrapingModule {}
  ```

#### 3.7 Testing Manual de Fase 3 (2 horas)
- [ ] **Crear endpoint de testing temporal**
  ```typescript
  @Post('test/scrape')
  async testScrape(@Body() body: { url: string }) {
    return this.advancedScrapingService.scrape(body.url, {
      selectors: { title: 'h1', content: '.content' },
      useProxy: false,
      useJavaScript: false,
      timeout: 30000,
    });
  }
  ```

- [ ] **Probar contra sitios reales**
  - [ ] Sitio sin protecciÃ³n (blog)
  - [ ] Sitio con Cloudflare
  - [ ] Sitio con JS rendering

- [ ] **Verificar logs** - Debe mostrar quÃ© mÃ©todo funcionÃ³

- [ ] **Build**
  ```bash
  yarn build
  ```

- [ ] **Commit**
  ```bash
  git add .
  git commit -m "feat: Phase 3 - Advanced anti-detection (rebrowser, crawlee, fallback chain)"
  git push origin feature/advanced-anti-detection
  ```

---

### FASE 4: CAPTCHA & MÃ‰TRICAS â±ï¸ 8-12 horas

#### 4.1 CaptchaSolverService (4-5 horas)
- [ ] **Crear servicio**
  ```bash
  touch src/scraping/services/captcha-solver.service.ts
  ```

- [ ] **Implementar integraciÃ³n con 2Captcha/NoCaptchaAI**
  ```typescript
  import { Injectable, Logger } from '@nestjs/common';
  import { ConfigService } from '@nestjs/config';

  @Injectable()
  export class CaptchaSolverService {
    private readonly logger = new Logger(CaptchaSolverService.name);
    private solver: any;

    constructor(private configService: ConfigService) {
      const apiKey = this.configService.get('TWOCAPTCHA_API_KEY');
      if (apiKey) {
        const Captcha = require('2captcha');
        this.solver = new Captcha.Solver(apiKey);
      }
    }

    async detectCaptcha(page: any): Promise<boolean> {
      const selectors = [
        'iframe[src*="recaptcha"]',
        'iframe[src*="hcaptcha"]',
        '.cf-turnstile',
        '#captcha',
        '.g-recaptcha',
      ];

      for (const selector of selectors) {
        const element = await page.$(selector);
        if (element) {
          this.logger.log(`CAPTCHA detected: ${selector}`);
          return true;
        }
      }

      return false;
    }

    async solveCaptcha(page: any, url: string): Promise<boolean> {
      if (!this.solver) {
        this.logger.warn('CAPTCHA solver not configured');
        return false;
      }

      // Detectar tipo de CAPTCHA
      const recaptcha = await page.$('iframe[src*="recaptcha"]');

      if (recaptcha) {
        return this.solveRecaptcha(page, url);
      }

      return false;
    }

    private async solveRecaptcha(page: any, url: string): Promise<boolean> {
      try {
        // Extraer site key
        const siteKey = await page.evaluate(() => {
          const iframe = document.querySelector('iframe[src*="recaptcha"]') as HTMLIFrameElement;
          return iframe?.src.match(/k=([^&]+)/)?.[1];
        });

        if (!siteKey) {
          this.logger.error('Could not extract reCAPTCHA site key');
          return false;
        }

        this.logger.log(`Solving reCAPTCHA for site key: ${siteKey}`);

        // Resolver con 2Captcha
        const result = await this.solver.recaptcha({
          pageurl: url,
          googlekey: siteKey,
        });

        // Inyectar token
        await page.evaluate((token: string) => {
          const textarea = document.getElementById('g-recaptcha-response') as HTMLTextAreaElement;
          if (textarea) {
            textarea.innerHTML = token;
            textarea.value = token;
          }
        }, result.data);

        this.logger.log('âœ… CAPTCHA solved successfully');
        return true;
      } catch (error) {
        this.logger.error(`Failed to solve CAPTCHA: ${error.message}`);
        return false;
      }
    }
  }
  ```

- [ ] **Integrar en AdvancedScrapingService**
  ```typescript
  constructor(
    // ... existentes
    private captchaSolver: CaptchaSolverService, // NUEVO
  ) {}

  // En scrapeWithRebrowser(), despuÃ©s de page.goto():
  const hasCaptcha = await this.captchaSolver.detectCaptcha(page);
  if (hasCaptcha) {
    this.logger.log('CAPTCHA detected, attempting to solve...');
    const solved = await this.captchaSolver.solveCaptcha(page, url);

    if (solved) {
      await this.randomDelay(2000, 4000); // Esperar despuÃ©s de resolver
      // Continuar con extracciÃ³n
    } else {
      throw new Error('Failed to solve CAPTCHA');
    }
  }
  ```

#### 4.2 Mejorar ScrapingMetricsService (2-3 horas)
- [ ] **Agregar persistencia (opcional - MongoDB)**
  ```typescript
  // Crear schema para mÃ©tricas
  @Schema()
  export class ScrapingMetric {
    @Prop({ required: true })
    domain: string;

    @Prop({ required: true })
    url: string;

    @Prop({ required: true, enum: ['success', 'failure'] })
    result: string;

    @Prop()
    method: string;

    @Prop()
    proxyUsed: string;

    @Prop()
    proxyType: string;

    @Prop({ default: false })
    captchaEncountered: boolean;

    @Prop({ default: false })
    blockedByCloudflare: boolean;

    @Prop()
    processingTime: number;

    @Prop({ default: Date.now })
    createdAt: Date;
  }
  ```

- [ ] **Agregar mÃ©todo para obtener mÃ©tricas agregadas**
  ```typescript
  async getOverallMetrics(): Promise<any> {
    const allMetrics = Array.from(this.metrics.values());

    const total = allMetrics.reduce((sum, m) => sum + m.totalRequests, 0);
    const successful = allMetrics.reduce((sum, m) => sum + m.successfulRequests, 0);

    return {
      overall: {
        totalRequests: total,
        successfulRequests: successful,
        failedRequests: total - successful,
        successRate: total > 0 ? (successful / total) * 100 : 0,
      },
      byDomain: Object.fromEntries(
        allMetrics.map(m => [
          m.domain,
          {
            totalRequests: m.totalRequests,
            successRate: m.totalRequests > 0
              ? (m.successfulRequests / m.totalRequests) * 100
              : 0,
            blockedByCloudflare: m.blockedByCloudflare,
          }
        ])
      ),
      last24Hours: await this.getLast24HoursMetrics(),
    };
  }
  ```

#### 4.3 API Endpoints para MÃ©tricas (1-2 horas)
- [ ] **Crear controller temporal o agregar a NoticiasController**
  ```typescript
  @Get('scraping/metrics/overview')
  async getScrapingMetrics() {
    return this.scrapingMetricsService.getOverallMetrics();
  }

  @Get('scraping/metrics/domain/:domain')
  async getDomainMetrics(@Param('domain') domain: string) {
    return this.scrapingMetricsService.getDomainMetrics(domain);
  }

  @Get('scraping/metrics/proxy-health')
  async getProxyHealth() {
    return this.proxyConfigurationService.getProxyHealthMetrics();
  }
  ```

#### 4.4 Registrar en MÃ³dulo (15 min)
- [ ] **Actualizar ScrapingModule**
  ```typescript
  import { CaptchaSolverService } from './services/captcha-solver.service';

  providers: [
    // ... existentes
    CaptchaSolverService, // NUEVO
  ],
  exports: [
    // ... existentes
    CaptchaSolverService, // NUEVO
  ],
  ```

#### 4.5 Testing Manual de Fase 4 (1-2 horas)
- [ ] **Configurar API key de CAPTCHA** (en .env)
  ```bash
  TWOCAPTCHA_API_KEY=tu_api_key_aqui
  ```

- [ ] **Probar contra sitio con CAPTCHA**
  - [ ] Verificar detecciÃ³n
  - [ ] Verificar resoluciÃ³n
  - [ ] Verificar inyecciÃ³n de token

- [ ] **Probar endpoints de mÃ©tricas**
  ```bash
  curl http://localhost:3000/scraping/metrics/overview
  curl http://localhost:3000/scraping/metrics/domain/example.com
  ```

- [ ] **Build**
  ```bash
  yarn build
  ```

- [ ] **Commit**
  ```bash
  git add .
  git commit -m "feat: Phase 4 - CAPTCHA solving and metrics dashboard"
  git push origin feature/advanced-anti-detection
  ```

---

### FASE 5: INTEGRACIÃ“N CON MÃ“DULOS EXISTENTES â±ï¸ 8-10 horas

#### 5.1 Modificar NoticiasModule (2-3 horas)
- [ ] **Importar ScrapingModule en NoticiasModule**
  ```typescript
  // packages/api-nueva/src/noticias/noticias.module.ts
  import { ScrapingModule } from '../scraping/scraping.module';

  @Module({
    imports: [
      // ... existentes
      ScrapingModule, // NUEVO
    ],
    // ... resto
  })
  export class NoticiasModule {}
  ```

- [ ] **Modificar NoticiasScrapingService**
  ```typescript
  // packages/api-nueva/src/noticias/services/noticias-scraping.service.ts
  import { AdvancedScrapingService } from '../../scraping/services/advanced-scraping.service';
  import { ScraperConfig, ScrapingResult } from '../../scraping/interfaces/scraping.interface';

  @Injectable()
  export class NoticiasScrapingService {
    constructor(
      // Mantener existentes
      @InjectModel(ExtractedNoticia.name) private extractedNoticiaModel: Model<ExtractedNoticia>,
      @InjectModel(NoticiasExtractionConfig.name) private configModel: Model<NoticiasExtractionConfig>,
      private cacheService: CacheService,
      private paginationService: PaginationService,
      // NUEVO
      private advancedScrapingService: AdvancedScrapingService,
    ) {}

    async extractFromUrl(
      url: string,
      config: NoticiasExtractionConfig,
    ): Promise<ScrapingResult> {
      // Cache check (mantener lÃ³gica existente)
      const cacheKey = this.generateCacheKey(url, config);
      const cached = await this.cacheService.get<ScrapingResult>(cacheKey);
      if (cached) {
        this.logger.log(`Cache HIT for ${url}`);
        return cached;
      }

      // NUEVO: Delegar al servicio avanzado
      const scraperConfig: ScraperConfig = {
        selectors: config.selectors,
        useProxy: config.extractionSettings.useProxy ?? false,
        useJavaScript: config.extractionSettings.useJavaScript ?? false,
        timeout: config.extractionSettings.timeout ?? 30000,
        retryAttempts: config.extractionSettings.retryAttempts ?? 3,
      };

      const result = await this.advancedScrapingService.scrape(url, scraperConfig);

      // Cache result
      await this.cacheService.set(cacheKey, result, 1800); // 30 min

      return result;
    }

    // MANTENER mÃ©todos auxiliares existentes para compatibilidad
    private generateCacheKey(url: string, config: any): string {
      // LÃ³gica existente
    }
  }
  ```

- [ ] **Verificar que no se rompan otros mÃ©todos del servicio**

#### 5.2 Modificar GeneratorProModule (2-3 horas)
- [ ] **Importar ScrapingModule**
  ```typescript
  // packages/api-nueva/src/generator-pro/generator-pro.module.ts
  import { ScrapingModule } from '../scraping/scraping.module';

  @Module({
    imports: [
      // ... existentes
      ScrapingModule, // NUEVO
    ],
    // ... resto
  })
  export class GeneratorProModule {}
  ```

- [ ] **Modificar NewsWebsiteService**
  ```typescript
  // packages/api-nueva/src/generator-pro/services/news-website.service.ts
  import { AdvancedScrapingService } from '../../scraping/services/advanced-scraping.service';
  import { ScraperConfig } from '../../scraping/interfaces/scraping.interface';

  @Injectable()
  export class NewsWebsiteService {
    constructor(
      // Mantener existentes
      @InjectModel(NewsWebsiteConfig.name) private websiteConfigModel: Model<NewsWebsiteConfig>,
      @InjectModel(ExtractedNoticia.name) private extractedNoticiaModel: Model<ExtractedNoticia>,
      // NUEVO
      private advancedScrapingService: AdvancedScrapingService,
    ) {}

    async extractNewsContent(
      url: string,
      websiteConfigId: string,
    ): Promise<any> {
      const config = await this.websiteConfigModel.findById(websiteConfigId);

      if (!config) {
        throw new Error('Website config not found');
      }

      // NUEVO: Usar servicio avanzado
      const scraperConfig: ScraperConfig = {
        selectors: config.contentSelectors,
        useProxy: true, // Siempre usar proxy en generator-pro
        useJavaScript: config.extractionSettings?.useJavaScript ?? false,
        timeout: config.extractionSettings?.timeout ?? 30000,
      };

      const result = await this.advancedScrapingService.scrape(url, scraperConfig);

      // Transformar a formato esperado y guardar
      return this.saveExtractedContent(result, config);
    }

    private async saveExtractedContent(result: any, config: any) {
      // LÃ³gica existente de guardado
    }
  }
  ```

- [ ] **Modificar UrlExtractionService** (para extracciÃ³n de listing pages)
  ```typescript
  async extractUrls(websiteConfigId: string) {
    const config = await this.websiteConfigModel.findById(websiteConfigId);

    // NUEVO: Usar servicio avanzado para scraping de listing page
    const result = await this.advancedScrapingService.scrape(config.listingUrl, {
      selectors: { content: 'body' }, // Extrae HTML completo
      useProxy: true,
      useJavaScript: config.extractionSettings?.useJavaScript ?? false,
      timeout: 30000,
    });

    // Parsear links del HTML
    const $ = cheerio.load(result.data.content || '');
    const links = $(config.listingSelectors.articleLinks)
      .map((i, el) => $(el).attr('href'))
      .get();

    // Resto de lÃ³gica existente (deduplicaciÃ³n, etc.)
  }
  ```

#### 5.3 Actualizar Configuraciones (1 hora)
- [ ] **Agregar campo useProxy a NoticiasExtractionConfig schema**
  ```typescript
  // packages/api-nueva/src/noticias/schemas/noticias-extraction-config.schema.ts
  @Prop({
    type: {
      // ... campos existentes
      useProxy: { type: Boolean, default: false }, // NUEVO
    },
  })
  extractionSettings: {
    useJavaScript: boolean;
    waitTime: number;
    rateLimit: number;
    timeout: number;
    retryAttempts: number;
    respectRobots: boolean;
    useProxy: boolean; // NUEVO
  };
  ```

- [ ] **Actualizar DTOs**
  ```typescript
  // noticias-extraction-config.dto.ts
  export class ExtractionSettingsDto {
    // ... existentes
    @IsBoolean()
    @IsOptional()
    useProxy?: boolean;
  }
  ```

#### 5.4 Testing Manual de IntegraciÃ³n (2-3 horas)
- [ ] **Crear configuraciÃ³n de prueba en Noticias**
  - [ ] Crear NoticiasExtractionConfig para sitio de prueba
  - [ ] Probar extracciÃ³n con useProxy: false
  - [ ] Probar extracciÃ³n con useProxy: true
  - [ ] Verificar que datos se guardan en ExtractedNoticia

- [ ] **Probar Generator-Pro**
  - [ ] Crear NewsWebsiteConfig para sitio de prueba
  - [ ] Probar extracciÃ³n de URLs (listing page)
  - [ ] Probar extracciÃ³n de contenido (article page)
  - [ ] Verificar pipeline completo

- [ ] **Verificar mÃ©tricas**
  - [ ] Verificar que se registran attempts
  - [ ] Verificar success/failure rates
  - [ ] Verificar que proxies se rotan

- [ ] **Verificar backward compatibility**
  - [ ] Configuraciones existentes deben seguir funcionando
  - [ ] Jobs existentes en queue deben procesarse

#### 5.5 Build de Fase 5 (30 min)
- [ ] **Build completo**
  ```bash
  yarn build
  ```
- [ ] **Verificar sin errores de TypeScript**
- [ ] **Verificar que servidor levanta**

- [ ] **Commit**
  ```bash
  git add .
  git commit -m "feat: Phase 5 - Integration with NoticiasModule and GeneratorProModule"
  git push origin feature/advanced-anti-detection
  ```

---

### FASE 6: TESTING COMPLETO & AJUSTES â±ï¸ 12-16 horas

#### 6.1 Configurar Proxies Reales (1-2 horas)
- [ ] **Registrarse en ProxyWing** (o proveedor elegido)
- [ ] **Obtener URLs de proxies**
  - [ ] Datacenter proxies (para empezar)
  - [ ] Residential proxies (si presupuesto permite)

- [ ] **Configurar en .env**
  ```bash
  DATACENTER_PROXY_URLS=http://user:pass@proxy1.proxywing.com:8080,http://user:pass@proxy2.proxywing.com:8080
  RESIDENTIAL_PROXY_URLS=http://user:pass@residential.proxywing.com:8080
  ```

- [ ] **Verificar conectividad de proxies**
  ```bash
  curl --proxy http://user:pass@proxy1.proxywing.com:8080 https://httpbin.org/ip
  ```

#### 6.2 Testing de Noticias Module (3-4 horas)
- [ ] **Sitios sin protecciÃ³n**
  - [ ] Seleccionar 5 sitios de noticias locales (MÃ©xico)
  - [ ] Crear configuraciones
  - [ ] Probar extracciÃ³n sin proxy
  - [ ] Verificar success rate > 95%

- [ ] **Sitios con protecciÃ³n ligera**
  - [ ] Seleccionar 3 sitios con Cloudflare bÃ¡sico
  - [ ] Crear configuraciones
  - [ ] Probar sin proxy â†’ DeberÃ­a fallar
  - [ ] Probar con proxy datacenter â†’ Puede funcionar
  - [ ] Probar con proxy residential â†’ Debe funcionar
  - [ ] Verificar success rate > 80%

- [ ] **Verificar datos extraÃ­dos**
  - [ ] Title completo y limpio
  - [ ] Content sin scripts/styles
  - [ ] Images correctamente extraÃ­das
  - [ ] publishedAt parseado
  - [ ] Keywords extraÃ­dos

- [ ] **Probar queue processing**
  - [ ] Agregar 50 URLs a la cola
  - [ ] Verificar que se procesan sin bloquear
  - [ ] Verificar logs de Ã©xito/fallo
  - [ ] Verificar mÃ©tricas actualizadas

#### 6.3 Testing de Generator-Pro Module (3-4 horas)
- [ ] **Configurar 2 sitios de noticias completos**
  - [ ] Sitio 1: Blog sin protecciÃ³n
  - [ ] Sitio 2: Sitio de noticias con Cloudflare

- [ ] **Probar extracciÃ³n de URLs**
  - [ ] Verificar que listing page se scrapea
  - [ ] Verificar que URLs se extraen correctamente
  - [ ] Verificar deduplicaciÃ³n (no duplicados en ExtractedUrlTracking)

- [ ] **Probar extracciÃ³n de contenido**
  - [ ] Verificar que cada URL se procesa
  - [ ] Verificar que datos se guardan en ExtractedNoticia
  - [ ] Verificar que status se actualiza

- [ ] **Probar pipeline completo**
  - [ ] Ejecutar runFullPipeline()
  - [ ] Verificar extraction â†’ generation â†’ publishing
  - [ ] Verificar logs en cada etapa

- [ ] **Probar manejo de errores**
  - [ ] URL invÃ¡lida â†’ debe registrar error
  - [ ] CAPTCHA no resuelto â†’ debe fallar con mensaje claro
  - [ ] Timeout â†’ debe reintentar

#### 6.4 Testing Contra Sitios Protegidos (2-3 horas)
- [ ] **Cloudflare**
  - [ ] Probar con sitio protegido por Cloudflare
  - [ ] Verificar bypass con rebrowser
  - [ ] Medir success rate (objetivo: > 80%)

- [ ] **CAPTCHA**
  - [ ] Probar con sitio que muestra CAPTCHA
  - [ ] Verificar detecciÃ³n
  - [ ] Verificar resoluciÃ³n (si API key configurada)
  - [ ] Medir success rate (objetivo: > 90%)

- [ ] **Rate Limiting**
  - [ ] Hacer 100 requests al mismo dominio
  - [ ] Verificar que se respeta rate limit
  - [ ] Verificar que no hay bloqueos

#### 6.5 OptimizaciÃ³n y Ajustes (2-3 horas)
- [ ] **Ajustar rate limiting**
  - [ ] Revisar mÃ©tricas de bloqueos
  - [ ] Si muchos bloqueos â†’ reducir velocidad
  - [ ] Si ningÃºn bloqueo â†’ puede incrementar

- [ ] **Ajustar proxy tiers**
  - [ ] Revisar quÃ© dominios necesitan proxies mejores
  - [ ] Ajustar lÃ³gica de escalaciÃ³n en ProxyConfigurationService

- [ ] **Optimizar timeouts**
  - [ ] Revisar tiempos de procesamiento promedio
  - [ ] Ajustar timeout defaults si es necesario

- [ ] **Optimizar cache TTL**
  - [ ] Revisar si 30 min es adecuado
  - [ ] Considerar TTL diferente para sitios que cambian mÃ¡s/menos

#### 6.6 DocumentaciÃ³n de Cambios (1 hora)
- [ ] **Actualizar README del proyecto**
  - [ ] Agregar secciÃ³n "Anti-Detection System"
  - [ ] Documentar nuevas variables de entorno
  - [ ] Documentar cÃ³mo configurar proxies

- [ ] **Documentar endpoints nuevos**
  - [ ] GET /scraping/metrics/overview
  - [ ] GET /scraping/metrics/domain/:domain
  - [ ] GET /scraping/metrics/proxy-health

- [ ] **Crear guÃ­a de troubleshooting**
  ```markdown
  ## Troubleshooting

  ### Cloudflare no se bypasea
  1. Verificar que rebrowser-patches estÃ¡ instalado
  2. Verificar env vars REBROWSER_PATCHES_*
  3. Probar con proxy residential

  ### Proxies no funcionan
  1. Verificar conectividad: curl --proxy ...
  2. Verificar credenciales en .env
  3. Verificar que ProxyConfigurationService carga proxies

  ### CAPTCHA no se resuelve
  1. Verificar API key de 2Captcha
  2. Verificar balance en cuenta
  3. Verificar logs de CaptchaSolverService
  ```

#### 6.7 Build Final de Fase 6 (30 min)
- [ ] **Build completo**
  ```bash
  yarn build
  ```
- [ ] **Testing de smoke test completo**
  - [ ] Levantar servidor
  - [ ] Probar 1 extracciÃ³n de noticias
  - [ ] Probar 1 extracciÃ³n de generator-pro
  - [ ] Verificar mÃ©tricas

- [ ] **Commit final**
  ```bash
  git add .
  git commit -m "feat: Phase 6 - Complete testing, adjustments, and documentation"
  git push origin feature/advanced-anti-detection
  ```

---

### FASE 7: DEPLOY A PRODUCCIÃ“N â±ï¸ 4-6 horas

#### 7.1 PreparaciÃ³n de Staging (1 hora)
- [ ] **Configurar proxies en staging**
  - [ ] Copiar URLs de proxies a .env de staging
  - [ ] Copiar API keys de CAPTCHA

- [ ] **Deploy a staging**
  ```bash
  # MÃ©todo segÃºn infraestructura actual
  git checkout feature/advanced-anti-detection
  git pull origin feature/advanced-anti-detection
  # Deploy script
  ```

- [ ] **Verificar que levanta sin errores**
  ```bash
  pm2 logs api-nueva
  # o mÃ©todo de logs actual
  ```

#### 7.2 Testing en Staging (2-3 horas)
- [ ] **Smoke tests**
  - [ ] Probar endpoint /health
  - [ ] Probar /scraping/metrics/overview
  - [ ] Probar extracciÃ³n de 1 noticia

- [ ] **Load test ligero**
  - [ ] Encolar 100 URLs
  - [ ] Monitorear CPU/memoria
  - [ ] Verificar que se procesan sin errores

- [ ] **Monitorear 24 horas**
  - [ ] Configurar alertas (si no existen)
  - [ ] Revisar mÃ©tricas cada 6 horas
  - [ ] Verificar logs de errores
  - [ ] Verificar que queues se procesan

#### 7.3 Deploy a ProducciÃ³n (1 hora)
- [ ] **Backup de producciÃ³n**
  ```bash
  mongodump --uri="mongodb://..." --out=backup-prod-$(date +%Y%m%d)
  redis-cli SAVE
  ```

- [ ] **Merge a main**
  ```bash
  git checkout main
  git pull origin main
  git merge feature/advanced-anti-detection
  git push origin main
  ```

- [ ] **Deploy en ventana de bajo trÃ¡fico**
  - [ ] Notificar al equipo
  - [ ] Ejecutar deploy
  - [ ] Verificar que levanta

#### 7.4 Post-Deploy Monitoring (1-2 horas)
- [ ] **Monitoring inmediato (primeras 2 horas)**
  - [ ] Revisar logs cada 15 min
  - [ ] Verificar mÃ©tricas de scraping
  - [ ] Verificar queue processing
  - [ ] Verificar uso de proxies
  - [ ] Verificar costos de CAPTCHA

- [ ] **Configurar alertas**
  - [ ] Alerta si success rate < 70%
  - [ ] Alerta si queue crece sin control
  - [ ] Alerta si costos de CAPTCHA > budget

- [ ] **Documentar mÃ©tricas baseline**
  ```
  Success rate Day 1: X%
  Avg processing time: X ms
  Proxies used: X requests
  CAPTCHAs solved: X
  Cost Day 1: $X
  ```

#### 7.5 Rollback Plan (si es necesario)
- [ ] **Si hay errores crÃ­ticos**
  ```bash
  git revert HEAD
  git push origin main
  # Re-deploy
  ```

- [ ] **Restaurar DB si es necesario**
  ```bash
  mongorestore backup-prod-YYYYMMDD/
  ```

---

## Resumen Ejecutivo por Fase

### FASE 0: PREPARACIÃ“N
**DuraciÃ³n:** 4-6 horas | **Build:** âœ… SÃ­

**Â¿QuÃ© se hace?**
- Backup de datos crÃ­ticos
- InstalaciÃ³n de dependencias (rebrowser, crawlee, bottleneck, proxies)
- ConfiguraciÃ³n de variables de entorno
- VerificaciÃ³n de build exitoso

**Â¿QuÃ© esperar?**
- Proyecto sigue funcionando igual
- Nuevas dependencias instaladas sin conflictos
- Build exitoso sin errores de TypeScript

**Â¿QuÃ© aprobar?**
âœ… Dependencias instaladas
âœ… Variables de entorno configuradas
âœ… Build exitoso

**Riesgos:**
ğŸŸ¡ Conflictos de versiones de dependencias (probabilidad baja)

---

### FASE 1: QUICK WINS
**DuraciÃ³n:** 5-6 horas | **Build:** âœ… SÃ­ | **Impacto:** +10-15% Ã©xito

**Â¿QuÃ© se hace?**
- Actualizar User-Agents a Chrome 131, Firefox 133 (2025)
- Agregar headers Sec-CH-UA (requeridos en 2025)
- Integrar rebrowser-puppeteer-core (bypass Cloudflare)
- Integrar Bottleneck (rate limiting avanzado)

**Â¿QuÃ© esperar?**
- Mejora inmediata de 10-15% en tasa de Ã©xito
- Headers modernos enviados en requests
- Rate limiting mÃ¡s inteligente
- Logs muestran UAs modernos

**Â¿QuÃ© aprobar?**
âœ… Headers Sec-CH-UA presentes en requests
âœ… User-Agents modernos (verificar en logs)
âœ… Rebrowser instalado y funcionando
âœ… Rate limiting funcional

**Riesgos:**
ğŸŸ¢ Bajo - Cambios mÃ­nimos, alta compatibilidad

---

### FASE 2: MÃ“DULO SCRAPINGMODULE BASE
**DuraciÃ³n:** 12-16 horas | **Build:** âœ… SÃ­ despuÃ©s de cada servicio

**Â¿QuÃ© se hace?**
- Crear estructura completa del mÃ³dulo ScrapingModule
- Implementar ProxyConfigurationService (rotaciÃ³n inteligente)
- Implementar FingerprintGeneratorService (fingerprints modernos)
- Implementar ScrapingMetricsService (tracking de Ã©xito/fallo)

**Â¿QuÃ© esperar?**
- MÃ³dulo nuevo funcionando de forma independiente
- Proxies se rotan automÃ¡ticamente (si configurados)
- MÃ©tricas disponibles via API
- Sistema preparado para siguiente fase

**Â¿QuÃ© aprobar?**
âœ… MÃ³dulo compila sin errores
âœ… Servicios funcionan individualmente (tests manuales)
âœ… ProxyConfigurationService selecciona proxies correctamente
âœ… MÃ©tricas se registran correctamente

**Riesgos:**
ğŸŸ¡ Medio - MÃ³dulo nuevo, pero aislado del resto

---

### FASE 3: ANTI-DETECCIÃ“N AVANZADA
**DuraciÃ³n:** 12-18 horas | **Build:** âœ… SÃ­

**Â¿QuÃ© se hace?**
- Implementar AdvancedScrapingService (core del sistema)
- Implementar scrapeWithCheerio() (mÃ©todo rÃ¡pido)
- Implementar scrapeWithRebrowser() (bypass Cloudflare)
- Implementar scrapeWithCrawlee() (mÃ¡xima automatizaciÃ³n)
- Implementar fallback chain (intentar mÃ©todos hasta que funcione)

**Â¿QuÃ© esperar?**
- Sistema completo de anti-detecciÃ³n funcionando
- Fallback automÃ¡tico si un mÃ©todo falla
- Cloudflare bypasseado con rebrowser
- Logs muestran quÃ© mÃ©todo funcionÃ³ para cada URL

**Â¿QuÃ© aprobar?**
âœ… ExtracciÃ³n exitosa de sitio sin protecciÃ³n (Cheerio)
âœ… ExtracciÃ³n exitosa de sitio con Cloudflare (Rebrowser)
âœ… Fallback chain funciona (si Cheerio falla, intenta Rebrowser)
âœ… Datos extraÃ­dos son correctos y completos

**Riesgos:**
ğŸŸ¡ Medio - LÃ³gica compleja, requiere testing exhaustivo

---

### FASE 4: CAPTCHA & MÃ‰TRICAS
**DuraciÃ³n:** 8-12 horas | **Build:** âœ… SÃ­

**Â¿QuÃ© se hace?**
- Implementar CaptchaSolverService (integraciÃ³n 2Captcha/NoCaptchaAI)
- Mejorar ScrapingMetricsService (persistencia, mÃ©tricas agregadas)
- Crear API endpoints para dashboard de mÃ©tricas

**Â¿QuÃ© esperar?**
- CAPTCHAs se detectan y resuelven automÃ¡ticamente
- MÃ©tricas completas por dominio, mÃ©todo, proxy
- Dashboard de mÃ©tricas accesible vÃ­a API
- Alertas si tasa de fallo > 50%

**Â¿QuÃ© aprobar?**
âœ… CAPTCHA detectado en sitio de prueba
âœ… CAPTCHA resuelto correctamente (si API key configurada)
âœ… MÃ©tricas disponibles en /scraping/metrics/overview
âœ… MÃ©tricas son precisas (comparar con logs)

**Riesgos:**
ğŸŸ¢ Bajo - Funcionalidades opcionales, no crÃ­ticas

---

### FASE 5: INTEGRACIÃ“N CON MÃ“DULOS EXISTENTES
**DuraciÃ³n:** 8-10 horas | **Build:** âœ… SÃ­ (CRÃTICO)

**Â¿QuÃ© se hace?**
- Modificar NoticiasScrapingService para usar AdvancedScrapingService
- Modificar NewsWebsiteService para usar AdvancedScrapingService
- Actualizar schemas para agregar campo useProxy
- Testing de backward compatibility

**Â¿QuÃ© esperar?**
- MÃ³dulos existentes usan nuevo sistema anti-detecciÃ³n
- Configuraciones existentes siguen funcionando (backward compatible)
- Datos se guardan en mismo formato que antes
- Mejora en tasa de Ã©xito visible en producciÃ³n

**Â¿QuÃ© aprobar?**
âœ… NoticiasModule funciona con nuevo sistema
âœ… GeneratorProModule funciona con nuevo sistema
âœ… Configuraciones existentes NO se rompen
âœ… Datos extraÃ­dos tienen mismo formato
âœ… Build exitoso sin errores
âœ… Servidor levanta sin errores

**Riesgos:**
ğŸ”´ Alto - Cambios en mÃ³dulos core, requiere testing exhaustivo

**CRITICAL:** Esta fase requiere aprobaciÃ³n explÃ­cita antes de continuar.

---

### FASE 6: TESTING COMPLETO & AJUSTES
**DuraciÃ³n:** 12-16 horas | **Build:** âœ… SÃ­

**Â¿QuÃ© se hace?**
- Configurar proxies reales (ProxyWing u otro proveedor)
- Testing extensivo de NoticiasModule (5-10 sitios)
- Testing extensivo de GeneratorProModule (2-3 sitios)
- Testing contra sitios protegidos (Cloudflare, CAPTCHA)
- OptimizaciÃ³n de rate limiting, timeouts, cache
- DocumentaciÃ³n completa

**Â¿QuÃ© esperar?**
- Sistema completamente funcional con proxies reales
- Success rate > 85% en sitios protegidos
- DocumentaciÃ³n completa de configuraciÃ³n
- GuÃ­a de troubleshooting
- Sistema listo para producciÃ³n

**Â¿QuÃ© aprobar?**
âœ… Success rate > 85% en testing
âœ… Cloudflare bypasseado consistentemente
âœ… CAPTCHAs resueltos > 90%
âœ… No hay memory leaks
âœ… DocumentaciÃ³n completa
âœ… Costos dentro de presupuesto ($30-50/mes)

**Riesgos:**
ğŸŸ¡ Medio - Requiere ajustes finos, puede tomar mÃ¡s tiempo

---

### FASE 7: DEPLOY A PRODUCCIÃ“N
**DuraciÃ³n:** 4-6 horas | **Build:** âœ… SÃ­

**Â¿QuÃ© se hace?**
- Deploy a staging
- Monitoring 24h en staging
- Deploy a producciÃ³n (ventana de bajo trÃ¡fico)
- Post-deploy monitoring intensivo
- ConfiguraciÃ³n de alertas

**Â¿QuÃ© esperar?**
- Sistema funcionando en producciÃ³n sin errores
- Mejora visible en success rate
- Costos controlados
- MÃ©tricas accesibles en dashboard

**Â¿QuÃ© aprobar?**
âœ… Staging funciona 24h sin errores crÃ­ticos
âœ… ProducciÃ³n deploy exitoso
âœ… Success rate > 85% en producciÃ³n
âœ… No hay errores en logs crÃ­ticos
âœ… Costos estÃ¡n dentro de presupuesto

**Riesgos:**
ğŸŸ¡ Medio - ProducciÃ³n siempre tiene riesgos

**ROLLBACK PLAN:** Preparado para revertir si hay errores crÃ­ticos

---

## MÃ©tricas de Ã‰xito

### MÃ©tricas TÃ©cnicas

| MÃ©trica | Actual | Objetivo Fase 1 | Objetivo Final |
|---------|--------|-----------------|----------------|
| **Success Rate** | 70% | 80-85% | 92%+ |
| **Cloudflare Bypass** | 0% | 50-60% | 87%+ |
| **Avg Extraction Time** | 5-10s | 4-8s | 3-5s |
| **CAPTCHA Solve Rate** | N/A | N/A | 95%+ |
| **URLs Processed/Day** | 1,000 | 1,500 | 10,000 |

### MÃ©tricas de Costos

| Concepto | Estimado Mes 1 | Estimado Mes 3 |
|----------|----------------|----------------|
| **Proxies** | $20-30 | $30-50 |
| **CAPTCHA** | $5-10 | $10-20 |
| **Infraestructura** | $0 (existente) | $0 |
| **TOTAL** | $25-40 | $40-70 |

### MÃ©tricas de Calidad

| MÃ©trica | Objetivo |
|---------|----------|
| **Data Completeness** | > 90% |
| **Title Quality** | > 95% |
| **Content Quality** | > 90% |
| **Image Extraction** | > 85% |

---

## ApÃ©ndice

### A. Comandos Ãštiles

```bash
# Build
yarn build

# Verificar que servidor levanta (no ejecutar)
yarn start:dev

# Verificar logs de queue
redis-cli KEYS "bull:*"

# Verificar mÃ©tricas de scraping
curl http://localhost:3000/scraping/metrics/overview

# Testing manual de proxy
curl --proxy http://user:pass@proxy.com:8080 https://httpbin.org/ip

# Verificar balance de 2Captcha
curl https://2captcha.com/res.php?key=YOUR_API_KEY&action=getbalance
```

### B. Variables de Entorno Completas

```bash
# Anti-Detection
DATACENTER_PROXY_URLS=
RESIDENTIAL_PROXY_URLS=
MOBILE_PROXY_URLS=
NOCAPTCHAAI_API_KEY=
TWOCAPTCHA_API_KEY=
SCRAPERAPI_KEY=
SCRAPERAPI_ENABLED=false

# Puppeteer
MAX_CONCURRENT_BROWSERS=5
BROWSER_POOL_SIZE=10
PUPPETEER_HEADLESS=true
PUPPETEER_TIMEOUT=30000

# Rate Limiting
DEFAULT_RATE_LIMIT=30
ADAPTIVE_RATE_LIMITING=true
RATE_LIMIT_PER_DOMAIN=true

# Monitoring
ENABLE_SCRAPING_METRICS=true
METRICS_RETENTION_DAYS=30
LOG_SCRAPING_DETAILS=true
ALERT_ON_FAILURE_RATE=0.5
```

### C. Proveedores Recomendados

**Proxies (Budget):**
- ProxyWing: https://proxywing.com ($2.50/GB residential)
- Webshare: https://webshare.io (10 free proxies)

**CAPTCHA:**
- NoCaptchaAI: https://nocaptchaai.com ($0.14/1000)
- 2Captcha: https://2captcha.com ($0.50/1000)

---

## Notas Finales

### Restricciones Recordadas
âœ… **SIN forwardRef** - Usar EventEmitter2 para comunicaciÃ³n entre mÃ³dulos
âœ… **SIN any types** - Tipado estricto TypeScript en todo momento
âœ… **SIN testing automÃ¡tico** - Testing manual por Coyotito
âœ… **Build despuÃ©s de cada fase** - Verificar que servidor levanta

### Puntos CrÃ­ticos
ğŸ”´ **FASE 5 es crÃ­tica** - Cambios en mÃ³dulos core, requiere aprobaciÃ³n explÃ­cita
ğŸ”´ **FASE 7 requiere ventana de bajo trÃ¡fico** - Coordinar con equipo
ğŸŸ¡ **Monitoreo post-deploy es crucial** - Primeras 48h crÃ­ticas

### PrÃ³ximos Pasos DespuÃ©s de ImplementaciÃ³n
1. Monitorear costos mensuales
2. Ajustar configuraciones basado en mÃ©tricas
3. Agregar mÃ¡s dominios a scraping
4. Considerar upgrade a proxies premium si necesario
5. Explorar ScraperAPI para dominios muy difÃ­ciles

---

**Fin del documento**

**VersiÃ³n:** 1.0
**Ãšltima actualizaciÃ³n:** 21 de Octubre de 2025
**PrÃ³xima revisiÃ³n:** DespuÃ©s de Fase 7 (deploy a producciÃ³n)
